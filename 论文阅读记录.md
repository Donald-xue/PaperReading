# 预取

## 二进制翻译器 + 预取

### 2009 基于动态二进制翻译的龙芯虚拟机中数据预取优化研究

采用软件查桩方式收集应用程序的访存指令其执行周期及步长变化信息来识别发生 Cache 缺失的延迟指令，并依此进行分类，接着对程序中的热代码构造数据预取优化单元——超级块(SuperBlock)，在此基础上实现了 SuperBlock 基本数据预取方案。

QEMU X86->MIPS

当某一基本块执行次数达到某一阈值条件时，将触发超级块的构造过程；对所形成的超级块，根据收集的信息进行数据预取分析，按照某些原则选择可能的需要进行预取的候选访存指令，然后在对超级块重新翻译当中对候选访存指令选择合适的数据预取指令。（hot tb   分析 + 预取）。

经过分析后通过将数据加载到 0 号寄存器完成数据预取优化。

本文插桩获取的信息很多，除了对每个tb的执行次数进行插桩计数以外，还包括访存指令的插桩：为了记录该数据加载指令的动态行为信息，包括访存指令每一次执行时的地址变化情况以及可能的 Cache 缺失所导致的时钟周期的统计结果。同基本块的插桩，对于每一条访存指令也都对应一个执行数据结构，该数据结构中包含有 3 个内存计数器、一个步长记录字段 (pre_stride)以及上一 次访存地址记录字段 (pre_addr)：1)exec_num 计数器，记录该访存指令的总执行次数；2)cycles 计数器，记录访存指令每次执行所需时钟周期数的累加统计值；3)neq_num 计数器，记录指令访存地址变化情况。

感觉似乎会有较大的插桩开销，文中说只有1%，**可以尝试一下**。不过文中的识别还是有不准确的缺陷。



## 硬件预取

#### 2024 Tyche: An Efficient and General Prefetcher for Indirect Memory Accesses

间接内存访问（ima，即A[f(B[i])]）由生产者-消费者对组成，其中消费者的内存地址来自生产者的内存数据。由于内置的值依赖特征，ima表现出较差的局部性，使得预取无效。

Tyche由三个主要组件组成：传播表（PT）、依赖链表（DCT）和地址生成队列（AGQ）。具体来说，PT负责前向传播源自跨步负载的指令依赖，以收集所有潜在的指令依赖链，而不管它们最终是否被消耗。在向前传播期间，如果遇到加载指令，DCT将触发向后传播，将链中的指令标记为有用，从而精确地提取IMA指令依赖链。最后，AGQ被设计成连续遍历依赖链并生成预取请求。

从一个跨步的访存指令作为开头前向传播，到一个非跨步的访存指令为止，从非跨步的访存指令进行后向传播，筛选出两条访存指令之间真正相关的指令依赖链。用一个单独的simple ALU对这个依赖链中的指令进行运算，再将算出来的地址交给AGQ发出预取请求。



## 软件预取

#### 1992 Design and Evaluation of a Compiler Algorithm for Prefetching

本文提出了一种将预取指令插入到密集矩阵代码中的编译算法。我们的算法识别那些可能是缓存丢失的引用，并且只对它们发出预取。

开发了一个编译器算法，使用局部性分析来选择性地只预取那些可能导致缓存丢失的引用。

在循环结构中适当位置插入预取指令，确保数据在访问时已被加载到缓存中。与软件流水线技术相结合，以隐藏预取引入的延迟。

分析了一些编译预取相关的细节，局部性分析、循环分解、预取安排等问题。

说实话没太读懂。



#### 2002 Efficient Discovery of Regular Stride Patterns in Irregular Programs and Its Use in Compiler Prefetching

该方法将跨幅信息分析和传统的边缘频率分析集成到一次分析中。

可以对SPECINT2000 中的181, 197, 254 子项的**链表**循环访问模式优化。

似乎在编译之前需要先分析出一个profile?

运行时，stride profile需要收集两种类型的信息：步幅值profile和步幅差profile。步幅差profile用于区分stride是phased（阶段性不变） stride还是alternated（交替） stride。

2. OVERVIEW OF PREFETCHING ALGORITHM

除了stride profile以外，还需要首先使用frequency profile来过滤掉显然不会从跨步预取中受益的负载（执行次数frequency少的）。

分析了三种stride profile feedback的可以预取的情况：Strong single stride (SSST) load，Phased multi-stride (PMST) load和Weak single stride (WSST) load。并对三种情况分别的识别和处理进行了比较详细的介绍。

171对应的应该是PMST？毕竟是几个数组之间来回进行访问。但同一个ld的话，似乎stride又是相同的？

3. EFFICIENT STRIDE PROFILING

如何减少profile的开销。关于采样进行profile和步幅预取小偏差仍有效的讨论。

讨论了一下将本文要用到的stride profie和编译器常见的frequency profile进行结合的方式，包括了block和edge frequency profile。以往都是先获得frequency profile，再进行stride profile。本文创新将两者结合再一个pass中完成。



# 机器学习+DBT

#### 2024 A System-Level Dynamic Binary Translator using Automatically-Learned Translation Rules

似乎机器学习+DBT的关键优化在于不需要IR的转化和人工的指令翻译，可以通过automatically-learned translation rules从guest isa pair直接翻译到host isa pair。减少了翻译过程中中间指令的转换过程。且减少了人力手工翻译的工作量。（用户态下）

但直接移植到原生QEMU系统态上会产生由于上下文切换带来的额外开销，还比不加优化慢了5%。本文针对这个问题提出了优化方法：

1. delay the parsing of the guest CPU state。    ->    reduce the number of needed instructions to maintain the guest CPU state during each context switch。
2. identify several common scenarios that can create rapid consecutive context switches and incur a substantial amount of overhead。包括连续内存访问、syscall导致的连续上下文切换等。通过合并其中的一些情况，以减少上下文切换的数量及其相关的开销。（系统级指令以及syscall都需要使用helper来完成。）
3. further: 为定义和使用CPU状态（例如条件代码/标志）的来宾指令做更好的代码调度，这可以减少维护这些CPU状态所需的冗余指令。

当应用所有优化后，可以消除48.83%与维护来宾CPU状态相关的所有操作（上下文切换中）。它可以实现比基准QEMU平均1.36倍的性能改进。



# Memory Access

#### 2005 QEMU, a Fast and Portable Dynamic Translator

1. QEMU在运行时进行翻译，并将翻译出的二进制代码存到code cache中，所以翻译出的代码可以被重用，与解释器相比的优势是guest指令仅仅需要被取和解码一次。
2. 将target CPU的指令分解为一些简单的微操作。微操作的数量比target指令的数目小很多。



问题：

1. QEMU constant parameters？是干什么用的，



文章中绿色为可能在latx-sys中添加的创新点，红色为存在疑问的地方。



#### 2015 Optimizing Memory Translation Emulation in Full System Emulators

类似于STLB优化技术的综述。

页表由模拟的x86 CR3寄存器指向。

*当STLB不包含地址空间标识符（asid）时，在客户操作系统中切换进程时必须刷新STLB。*

本文对一些可能的STLB优化进行了尝试和总结，包括硬件TLB的适配优化和STLB灵活性带来的特有的优化。包括：

硬件相关优化：

受害者（victim）STLB使用一个小的、完全关联的STLB来捕获直接映射的主STLB的一些冲突缺失。

set-associative（组相联） STLB改变了主STLB，使它在每个set中包含多个表项，以减少冲突丢失。本文考虑了集合关联STLB的几种变体，包括使用最新Intel处理器的SIMD指令扩展来加速搜索的一种变体。

软件优化：

bump-the-STLB分配技术：使用一个由多个STLB组成的池和一个单独的线程将STLB冲洗
从关键路径中取出。（latx中的平行flush？）

翻译合并（Translation coalescing， XC）：XC对于堆栈访问特别有效，push和pop序列。

超级页STLB缓存超级页的翻译，否则超级页在QEMU中被视为相邻的规则大小页面的集合。基于分析的超级页查找通过动态分析和将每个模拟内存指令与针对最可能使用的页面大小进行优化的STLB查找序列相关联，进一步改进了超级页处理。这避免了需要频繁地进行多个探测（每个探测具有不同的页面大小）才能找到相应的翻译。



#### 2015 HSPT: Practical Implementation and Efficient Management of Embedded Shadow Page Tables for Cross-ISA System Virtual Machines

ESPT：直接映射GVA -> HPA。

缺点：需要内核模块loadable kernel module(LKM)。

本文方法不采用LKM，而是使用共享内存映射方案，仅使用“mmap”系统调用来维护影子页表（SPT）。且更详细地研究了SPT对多处理的支持。SPT操作分为3个部分：

1. 创建SPT：使用主机页表的一部分作为SPT来完成创建SPT的操作。
2. 同步SPT：使用共享内存映射方案，其中多个虚拟页可以映射到同一物理页，以同步SPT和GPT。
3. 切换进程SPT：使用共享SPT来处理客户操作系统中的多处理。提出并评估了三种SPT组织，包括共享、私有和组共享SPT来处理客户操作系统中的多进程。提供了如何选择每种变体的指南。

未进行寄存器映射，寄存器存在内存中，所以guest内存访问导致一次host的页表加载和一次Guest的页表加载，所以本文将GPT和HPT和在以其存放。

本文是在64bit host里面运行32bit guest（64位也可以）。采用类似于直接映射窗口的技术，将guest memory+offset映射到一段host memory中，将4步转换减少为一步（GVA+offset(=HVA) -> HPA）。

创建SPT：

通过文件内偏移的方式将softmmu中的GVA和HSPT中的（GVA+offset）mmap到同一个GPA。

同步SPT：

(1) guestOS 创建新页表：lazy同步的方式，先仅仅创建一个没有访问权限的空表，访问到相应表项触发SIGSEGV，在信号处理函数中进行同步。

(2)guestOS更改：同样进行lazy同步，检查到TLB-invalid指令就将SPT清掉，填入没有访问权限的项。

多核切换支持：

(1) 共享SPT：每次切换进程时要把SPT中的项目全部刷掉。切换时进行预取优化减小开销。在每次填写SPT项的时候进行记录，下次切换回来的时候将记录下来的项进行同步。

(2) 私有SPT：开销小，但占用更大的HVA space。在切换进程的时候只需要更改HVA映射space的基址即可。

(3) 组相关SPT：综合以上两种，一定数量的SPT，SPT不够时根据LRU原则换出。





#### 2021 BTMMU: An Efficient and Versatile Cross-ISA Memory Virtualization

与之前的解决方案相比（如ESPT和HSPT），不需要要求host比guest有更大的地址空间。

本文：

1. 通过Dual-TLB的在每个TLB表项中添加一个两位的机器标识符 Machine IDentifier (MID) 的机制来扩展HVA space。

Dual-TLB:处理器为每次内存访问维护一个MID上下文。只有当当前访问的MID与MID上下文匹配时，才会命中TLB表项。MID有效地为进程提供了四个并发虚拟地址空间。

2. 对于不同的page size，采用软件管理的多种页表大小的MIPS MMU。

BTMMU在其内核模块中具有定制的影子页表结构，以便完全独立于主机页表处理。而ESPT将影子页表嵌入到主机页表中，限制了其适用范围。

在我们的设计中，主机操作系统将处理与HVA相关的异常，BTMMU内核模块将处理与GTLB相关的异常。guest有自己的专用例外处理入口。使用**硬件TLB**来加速地址访问，疑似hamt的mips前身。

基于guest pt base使用多SPT优化防止频繁刷新。与HSPT的multi方案相比，不需要占用大量HVA，且不依赖于guest asid。



## 软件页表缓存

#### 2010 Translation Caching: Skip, Don’t Walk (the Page Table)

本文讨论了几种TLB miss之后的页表缓存结构效果。

基数树页面表？本文表明，由于虚拟地址使用中的局部性，与基于哈希的表相比，基数表导致的总内存访问最多减少20%，DRAM访问最多减少400%。

对于名义大小的应用程序，TLB缺失对整体系统性能的影响在5-14%之间，即使在非虚拟化环境中也是如此。随着应用程序内存占用的增加，TLB缺失对性能的影响显著增大，在某些情况下接近50%。

缓存加速了在翻译TLB中出现miss后发生的页表遍历。本文表明，最有效的MMU缓存是Translation Cache，它存储部分地址翻译并允许页遍历硬件跳过页表的一个或多个级别。

缓存页遍历 CACHING PAGEWALKS：

1. 页表缓存 Page table caches：每个缓存表项对应单独一级的虚拟地址和物理地址的转换。每一级都分别存储。

2. 翻译缓存 Translation caches：通过完整的单级或多级虚拟地址作为索引，不需要从一个条目中获取数据来查找页表下一个较低级别的条目。所有的查找都可以彼此独立地执行。最后，MMU将选择与虚拟地址的最长前缀匹配的条目，因为它允许页面遍历跳过大多数级别。
3. 平移路径缓存：将以上两种方法结合。

文中给出了以上三种缓存的SPEC2006 fp测试数据，对统一和分割都进行了性能的比较，还有一些替代基数树页表的方案也进行了讨论。结论是认为统一翻译缓存效果最好。



## 内存映射

#### 2024 On-Demand Triggered Memory Management Unit in Dynamic Binary Translator

主要讨论了guest和host系统不同大小页面的情况下的安全问题。

用户态DBT中由于没有软件MMU，线性映射页面时如果host和guest页面大小不同会产生正确性和安全问题。用户级dbt利用线性映射，高性能，但有潜在的风险。系统级dbt采用软件MMU，消除了风险，但性能较差。我们提出了一种结合他们各自优势的新的内存管理方法，称为按需触发式内存管理单元（ODT-MMU），它将线性映射方法与风险出现时触发软件内存管理单元相结合。

我们以两种方式实现ODT-MMU: ODT-InterpMMU，一种解释风险页面访问的平台无关实现；ODT-ManipTLB，利用可编程TLB来增强风险页面访问性能。

将有风险的页面放到一个影子页表里面，访问时触发OS信号，用软件MMU单独处理。



# Code Cache

#### 2002 Code Cache Management Schemes for Dynamic Optimizers

可以优化例如Java这种运行时使用二进制码的语言。

本文评估了几种替代缓存管理方案，这些方案仅识别和删除足够的trace（而不是全部），以便为新trace腾出空间。现存的大部分缓存都是直接将整个cache进行刷新。

？本文使用循环缓冲区？我们可以将代码缓存未命中率降低一半。 此外，这种方法增加的额外开销非常小，并且避免了与代码缓存碎片相关的问题。

要找好缓存管理策略复杂性与其优化收益之间的平衡，防止过于复杂导致性能优化适得其反。

适用于动态翻译器和基于硬件的代码缓存机制 



#### 系统级二进制翻译器中软件代码缓存研究    师兄论文

1. 本文分析了系统级二进制翻译器中已有的指令语义模拟方案给软件代码缓存管理带来的问题，并将其总结为冲突问题和冗余问题;
2. 针对冲突问题，本文提出了多状态代码缓存方案，避免了不同状态的翻译代码块之间的冲突，降低了不必要的刷新，减少了动态翻译开销;
3. 针对冗余问题，本文提出了弱状态标签方案，消除了部分状态导致的冗余代码块，降低了翻译开销和内存占用，仅带来了非常有限的性能损失;



# 硬件辅助加速

#### Captive

The key idea is to eliminate performance bottlenecks by exploiting the existing virtualization hardware extensions originally devised for same-architecture virtualization.

论文内容：

1. We show how virtual-to-physical address translation can be accelerated through the use of virtualization extensions by mapping behavior of the guest MMU onto corresponding behavior of the host MMU.（hamt？）
2. We present a DBT system for the translation from the guest to host ISA, where a fast, block-based just-in-time (JIT) compiler that lives inside the native virtual machine compiles guest basic blocks to host native code.（使用到了kvm？）
3. We develop an efficient mechanism to emulate the guest’s memory mapped I/O devices by exploiting the MMU to detect device accesses.（设备直通？）
4. Finally, we devise an interrupt handling scheme, which correctly honors the guest’s instruction boundaries, even if one guest instruction is mapped onto several host instructions, thus implementing precise, yet efficient, guest interrupts.（guest中断？）



#  热路径优化

#### IA-32

首先翻译一次cold code, 设置一个运行时的计数器，在计数器达到阈值之后，将热点代码重新进行hot code的大量优化的翻译。



# Peephole Optimization

#### Performance Improvements via Peephole Optimization in Dynamic Binary Translation

1. 使用instptn进行优化: pxor, mulsd+addsd（主要是由于helper的上下文切换次数减少导致的性能优化？）
2. 使用live variable analysis进行活性分析，对于连续的内存访问的情况进行分析，减少LAL, LAS, SAS, SAL (L=Load, A=After, S=Store)的冗余内存访问情况。



# 自修改代码

#### Handling Self­Modifying Code Using Software Dynamic Translation

使用写保护的页面来处理自修改代码，将翻译后的cache地址进行写保护，如果对该地址写入则触发segv信号，然后在信号处理函数中进行处理（打开权限并写入，无效，重翻译），然后再继续执行。（似乎是用户态DBT）。

应该不是优化，而是一种对于smc的解决方案。



#### 2015 Optimizing Binary Translation of Dynamically Generated Code



#### 2018 Improving Dynamically-Generated Code Performance on Dynamic Binary Translators

王大哥工作复现的优化，用户态。

通过最大化已翻译tb的重用情况来减少**重翻译的开销**。



# 机器学习课，大语言模型

#### Debating with More Persuasive LLMs Leads to More Truthful Answers

*QuALITY comprehension task* 是一个用于评估人工智能系统（尤其是语言模型）对长篇、多段文本的阅读理解能力的基准测试任务（benchmark task）。

*QuALITY* 全称是 *"Question Answering with Long Input Texts, Yes!"*，由 Allen Institute for AI 提出，是一个旨在推动 AI 在复杂阅读理解方面进步的开源数据集和任务。它的重点是评估模型是否能真正“理解长文”，而不是靠关键词匹配或浅层推理。

任务特点：

1. 长文档输入（up to thousands of tokens）：
   - 文本包括小说、纪实文学、科技说明文等，有时篇幅多达几千词。
2. 复杂、多层次的问题：
   - 问题往往涉及跨段落推理、整合信息、理解主旨或细节。
3. 多项选择题格式（Multiple Choice QA）：
   - 每个问题提供一个上下文文档、一个问题以及多个选项（通常是4个），只有一个是正确答案。
4. 对模型“理解力”要求高：
   - 简单的词汇匹配或句子相似度方法很难得高分，需要真正“通读并理解”材料。



本文研究用一个LLM non-expert模型来判断两个expert模型给出的答案的正确性。两个expert模型argue for 自己不同的答案就成为辩论debate。debate可以帮助non-expert模型以较高的正确性回答QuALITY comprehension task。进一步的，优化expert模型的Persuasive可以增加non-expert判断的正确性。



*inference-time methods（推理时方法）* 指的是 在模型已经训练完毕之后，在实际使用或推理（inference）阶段应用的一些技术或策略，而不是在训练过程中对模型参数进行优化。

